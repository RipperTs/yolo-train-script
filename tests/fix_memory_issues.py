#!/usr/bin/env python3
"""
å†…å­˜é—®é¢˜å¿«é€Ÿä¿®å¤è„šæœ¬
è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜å ç”¨æŒç»­å¢å¤§çš„é—®é¢˜
"""

import sys
import gc
import torch
import psutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from memory_optimizer import memory_optimizer, training_memory_manager
from config_manager import config_manager


def check_current_memory():
    """æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("ğŸ” æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ...")
    
    memory_info = memory_optimizer.get_memory_info()
    
    print(f"ğŸ’¾ ç³»ç»Ÿå†…å­˜: {memory_info['ram_used_gb']:.1f}/{memory_info['ram_total_gb']:.1f}GB ({memory_info['ram_percent']:.1f}%)")
    
    if 'gpu_allocated_gb' in memory_info:
        print(f"ğŸ® GPUå†…å­˜: {memory_info['gpu_allocated_gb']:.1f}/{memory_info['gpu_total_gb']:.1f}GB ({memory_info['gpu_memory_percent']:.1f}%)")
    
    # å†…å­˜ä½¿ç”¨è¯„ä¼°
    if memory_info['ram_percent'] > 85:
        print("âš ï¸ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼")
        return 'high'
    elif memory_info['ram_percent'] > 70:
        print("ğŸŸ¡ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜")
        return 'medium'
    else:
        print("âœ… ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡æ­£å¸¸")
        return 'normal'


def force_memory_cleanup():
    """å¼ºåˆ¶å†…å­˜æ¸…ç†"""
    print("ğŸ§¹ æ‰§è¡Œå¼ºåˆ¶å†…å­˜æ¸…ç†...")
    
    # Pythonåƒåœ¾å›æ”¶
    collected = gc.collect()
    print(f"   Pythonåƒåœ¾å›æ”¶: æ¸…ç†äº† {collected} ä¸ªå¯¹è±¡")
    
    # GPUå†…å­˜æ¸…ç†
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print("   CUDAå†…å­˜ç¼“å­˜å·²æ¸…ç†")
    
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
        print("   MPSå†…å­˜ç¼“å­˜å·²æ¸…ç†")
    
    # å†æ¬¡åƒåœ¾å›æ”¶
    gc.collect()
    
    print("âœ… å†…å­˜æ¸…ç†å®Œæˆ")


def optimize_training_config():
    """ä¼˜åŒ–è®­ç»ƒé…ç½®ä»¥å‡å°‘å†…å­˜ä½¿ç”¨"""
    print("âš™ï¸ ä¼˜åŒ–è®­ç»ƒé…ç½®...")
    
    current_config = config_manager.get_training_config()
    memory_info = memory_optimizer.get_memory_info()
    
    optimizations = {}
    
    # æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´é…ç½®
    if memory_info['ram_percent'] > 80:
        # å†…å­˜ç´§å¼ ï¼Œæ¿€è¿›ä¼˜åŒ–
        optimizations.update({
            'batch_size': max(1, current_config.get('batch_size', 16) // 2),
            'workers': min(2, current_config.get('workers', 8)),
            'cache': False,
            'save_period': max(20, current_config.get('save_period', 10)),
        })
        print("   åº”ç”¨æ¿€è¿›å†…å­˜ä¼˜åŒ–é…ç½®")
    elif memory_info['ram_percent'] > 70:
        # å†…å­˜è¾ƒé«˜ï¼Œæ¸©å’Œä¼˜åŒ–
        optimizations.update({
            'batch_size': max(4, current_config.get('batch_size', 16) * 3 // 4),
            'workers': min(4, current_config.get('workers', 8)),
            'cache': False,
        })
        print("   åº”ç”¨æ¸©å’Œå†…å­˜ä¼˜åŒ–é…ç½®")
    
    # è®¾å¤‡ç‰¹å®šä¼˜åŒ–
    device = current_config.get('device', 'cpu')
    if device == 'mps':
        optimizations.update({
            'amp': False,
            'pin_memory': False,
            'persistent_workers': True,
        })
        print("   åº”ç”¨MPSè®¾å¤‡ä¼˜åŒ–")
    elif device.startswith('cuda'):
        # æ£€æŸ¥GPUå†…å­˜
        if memory_info.get('gpu_memory_percent', 0) > 80:
            optimizations.update({
                'batch_size': max(1, current_config.get('batch_size', 16) // 2),
                'amp': True,  # å¯ç”¨æ··åˆç²¾åº¦èŠ‚çœæ˜¾å­˜
            })
            print("   åº”ç”¨CUDAè®¾å¤‡å†…å­˜ä¼˜åŒ–")
    else:  # CPU
        optimizations.update({
            'amp': False,
            'pin_memory': False,
            'workers': min(2, current_config.get('workers', 8)),
        })
        print("   åº”ç”¨CPUè®¾å¤‡ä¼˜åŒ–")
    
    # åº”ç”¨ä¼˜åŒ–é…ç½®
    if optimizations:
        config_manager.update_training_config(**optimizations)
        print(f"âœ… é…ç½®ä¼˜åŒ–å®Œæˆ: {optimizations}")
    else:
        print("âœ… å½“å‰é…ç½®å·²æ˜¯æœ€ä¼˜")
    
    return optimizations


def kill_zombie_processes():
    """æ¸…ç†å¯èƒ½çš„åƒµå°¸è¿›ç¨‹"""
    print("ğŸ” æ£€æŸ¥å¹¶æ¸…ç†åƒµå°¸è¿›ç¨‹...")
    
    try:
        current_pid = psutil.Process().pid
        python_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['name'] and 'python' in proc.info['name'].lower():
                    if proc.info['pid'] != current_pid:  # ä¸åŒ…æ‹¬å½“å‰è¿›ç¨‹
                        cmdline = ' '.join(proc.info['cmdline'] or [])
                        if any(keyword in cmdline for keyword in ['trainer.py', 'smart_trainer.py', 'gradio_app.py']):
                            python_processes.append(proc)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        if python_processes:
            print(f"   å‘ç° {len(python_processes)} ä¸ªç›¸å…³Pythonè¿›ç¨‹")
            for proc in python_processes:
                try:
                    print(f"   è¿›ç¨‹ {proc.pid}: {' '.join(proc.cmdline())}")
                except:
                    print(f"   è¿›ç¨‹ {proc.pid}: æ— æ³•è·å–å‘½ä»¤è¡Œ")
            
            response = input("æ˜¯å¦ç»ˆæ­¢è¿™äº›è¿›ç¨‹ï¼Ÿ(y/N): ")
            if response.lower() == 'y':
                for proc in python_processes:
                    try:
                        proc.terminate()
                        print(f"   å·²ç»ˆæ­¢è¿›ç¨‹ {proc.pid}")
                    except:
                        print(f"   æ— æ³•ç»ˆæ­¢è¿›ç¨‹ {proc.pid}")
        else:
            print("   æœªå‘ç°ç›¸å…³è¿›ç¨‹")
    
    except Exception as e:
        print(f"   æ£€æŸ¥è¿›ç¨‹æ—¶å‡ºé”™: {e}")


def apply_emergency_fixes():
    """åº”ç”¨ç´§æ€¥ä¿®å¤æªæ–½"""
    print("ğŸš¨ åº”ç”¨ç´§æ€¥å†…å­˜ä¿®å¤æªæ–½...")
    
    # 1. å¼ºåˆ¶åƒåœ¾å›æ”¶
    for i in range(3):
        collected = gc.collect()
        print(f"   ç¬¬{i+1}æ¬¡åƒåœ¾å›æ”¶: æ¸…ç†äº† {collected} ä¸ªå¯¹è±¡")
    
    # 2. æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        # é‡ç½®CUDAä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            torch.cuda.reset_peak_memory_stats()
            print("   CUDAå†…å­˜ç»Ÿè®¡å·²é‡ç½®")
        except:
            pass
    
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    
    # 3. è®¾ç½®æœ€ä¿å®ˆçš„é…ç½®
    emergency_config = {
        'batch_size': 1,
        'workers': 0,
        'cache': False,
        'amp': False,
        'pin_memory': False,
        'save_period': 50,
        'patience': 10,
    }
    
    config_manager.update_training_config(**emergency_config)
    print(f"   åº”ç”¨ç´§æ€¥é…ç½®: {emergency_config}")
    
    print("âœ… ç´§æ€¥ä¿®å¤æªæ–½å·²åº”ç”¨")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ å†…å­˜é—®é¢˜å¿«é€Ÿä¿®å¤å·¥å…·")
    print("=" * 50)
    
    # 1. æ£€æŸ¥å½“å‰å†…å­˜çŠ¶æ€
    memory_status = check_current_memory()
    print()
    
    # 2. å¼ºåˆ¶å†…å­˜æ¸…ç†
    force_memory_cleanup()
    print()
    
    # 3. æ£€æŸ¥æ¸…ç†åçš„å†…å­˜çŠ¶æ€
    print("ğŸ” æ¸…ç†åå†…å­˜çŠ¶æ€:")
    memory_status_after = check_current_memory()
    print()
    
    # 4. ä¼˜åŒ–è®­ç»ƒé…ç½®
    optimizations = optimize_training_config()
    print()
    
    # 5. æ£€æŸ¥åƒµå°¸è¿›ç¨‹
    kill_zombie_processes()
    print()
    
    # 6. å¦‚æœå†…å­˜ä»ç„¶å¾ˆé«˜ï¼Œåº”ç”¨ç´§æ€¥ä¿®å¤
    if memory_status_after == 'high':
        apply_emergency_fixes()
        print()
    
    # 7. æœ€ç»ˆæ£€æŸ¥
    print("ğŸ” æœ€ç»ˆå†…å­˜çŠ¶æ€:")
    final_status = check_current_memory()
    print()
    
    # 8. ç»™å‡ºå»ºè®®
    print("ğŸ’¡ å»ºè®®:")
    if final_status == 'normal':
        print("âœ… å†…å­˜çŠ¶æ€å·²æ¢å¤æ­£å¸¸ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
        print("ğŸ“ å»ºè®®å¯ç”¨å†…å­˜ç›‘æ§: python memory_monitor.py --action monitor")
    elif final_status == 'medium':
        print("ğŸŸ¡ å†…å­˜çŠ¶æ€æœ‰æ‰€æ”¹å–„ï¼Œå»ºè®®:")
        print("   - ä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡å¤§å°")
        print("   - å¯ç”¨å†…å­˜ç›‘æ§")
        print("   - è€ƒè™‘åˆ†æ‰¹è®­ç»ƒ")
    else:
        print("ğŸ”´ å†…å­˜çŠ¶æ€ä»ç„¶è¾ƒé«˜ï¼Œå»ºè®®:")
        print("   - é‡å¯ç³»ç»Ÿ")
        print("   - å…³é—­å…¶ä»–åº”ç”¨ç¨‹åº")
        print("   - ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
        print("   - è€ƒè™‘ä½¿ç”¨äº‘ç«¯è®­ç»ƒ")
    
    print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹: docs/å†…å­˜ä¼˜åŒ–æŒ‡å—.md")


if __name__ == "__main__":
    main()
