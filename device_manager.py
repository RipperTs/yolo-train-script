"""
è®¾å¤‡ç®¡ç†æ¨¡å—
æ™ºèƒ½æ£€æµ‹å’Œç®¡ç†CPU/GPUè®¾å¤‡
"""

import torch
import platform
from typing import List, Dict, Optional


class DeviceManager:
    """è®¾å¤‡ç®¡ç†å™¨"""
    
    def __init__(self):
        self.available_devices = self._detect_devices()
        self.current_device = self._get_default_device()
    
    def _detect_devices(self) -> List[Dict]:
        """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        devices = []
        
        # æ·»åŠ CPUè®¾å¤‡
        devices.append({
            "name": "CPU",
            "type": "cpu",
            "id": "cpu",
            "memory": self._get_cpu_memory(),
            "available": True,
            "description": f"CPU ({platform.processor() or 'Unknown'})"
        })
        
        # æ£€æµ‹CUDAè®¾å¤‡
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                try:
                    device_name = torch.cuda.get_device_name(i)
                    device_memory = torch.cuda.get_device_properties(i).total_memory
                    devices.append({
                        "name": f"GPU {i}",
                        "type": "cuda",
                        "id": f"cuda:{i}",
                        "memory": device_memory,
                        "available": True,
                        "description": f"GPU {i}: {device_name} ({device_memory // (1024**3)}GB)"
                    })
                except Exception as e:
                    print(f"æ£€æµ‹GPU {i}æ—¶å‡ºé”™: {e}")
        
        # æ£€æµ‹MPSè®¾å¤‡ (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            devices.append({
                "name": "MPS",
                "type": "mps", 
                "id": "mps",
                "memory": "å…±äº«å†…å­˜",
                "available": True,
                "description": "Apple Metal Performance Shaders (MPS)"
            })
        
        return devices
    
    def _get_cpu_memory(self) -> str:
        """è·å–CPUå†…å­˜ä¿¡æ¯"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            return f"{memory.total // (1024**3)}GB"
        except:
            return "Unknown"
    
    def _get_default_device(self) -> str:
        """è·å–é»˜è®¤è®¾å¤‡"""
        # ä¼˜å…ˆçº§: CUDA > MPS > CPU
        for device in self.available_devices:
            if device["type"] == "cuda" and device["available"]:
                return device["id"]
        
        for device in self.available_devices:
            if device["type"] == "mps" and device["available"]:
                return device["id"]
        
        return "cpu"
    
    def get_device_choices(self) -> List[str]:
        """è·å–è®¾å¤‡é€‰æ‹©åˆ—è¡¨"""
        return [device["id"] for device in self.available_devices if device["available"]]
    
    def get_device_descriptions(self) -> List[str]:
        """è·å–è®¾å¤‡æè¿°åˆ—è¡¨"""
        return [device["description"] for device in self.available_devices if device["available"]]
    
    def get_device_info(self, device_id: str) -> Optional[Dict]:
        """è·å–æŒ‡å®šè®¾å¤‡ä¿¡æ¯"""
        for device in self.available_devices:
            if device["id"] == device_id:
                return device
        return None
    
    def is_gpu_available(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨"""
        return any(device["type"] in ["cuda", "mps"] for device in self.available_devices)
    
    def get_device_status(self) -> Dict:
        """è·å–è®¾å¤‡çŠ¶æ€ä¿¡æ¯"""
        status = {
            "current_device": self.current_device,
            "available_devices": len(self.available_devices),
            "gpu_available": self.is_gpu_available(),
            "devices": []
        }
        
        for device in self.available_devices:
            device_status = device.copy()
            
            # æ·»åŠ å®æ—¶çŠ¶æ€ä¿¡æ¯
            if device["type"] == "cuda":
                try:
                    torch.cuda.set_device(device["id"])
                    device_status["memory_used"] = torch.cuda.memory_allocated()
                    device_status["memory_cached"] = torch.cuda.memory_reserved()
                    device_status["temperature"] = self._get_gpu_temperature(device["id"])
                except:
                    device_status["memory_used"] = "Unknown"
                    device_status["memory_cached"] = "Unknown"
                    device_status["temperature"] = "Unknown"
            
            status["devices"].append(device_status)
        
        return status
    
    def _get_gpu_temperature(self, device_id: str) -> str:
        """è·å–GPUæ¸©åº¦ï¼ˆå¦‚æœæ”¯æŒï¼‰"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ GPUæ¸©åº¦æ£€æµ‹é€»è¾‘
            # ä¸åŒçš„GPUå‚å•†æœ‰ä¸åŒçš„API
            return "Unknown"
        except:
            return "Unknown"
    
    def set_device(self, device_id: str, auto_fallback: bool = True) -> bool:
        """è®¾ç½®å½“å‰è®¾å¤‡ï¼Œæ”¯æŒè‡ªåŠ¨é™çº§"""
        # é¦–å…ˆéªŒè¯è®¾å¤‡æ˜¯å¦çœŸå®å¯ç”¨
        validation_result = self.validate_device_availability(device_id)
        
        if validation_result["available"]:
            # è®¾å¤‡å¯ç”¨ï¼Œç›´æ¥è®¾ç½®
            self.current_device = device_id
            
            # è®¾ç½®PyTorché»˜è®¤è®¾å¤‡
            try:
                if device_id.startswith("cuda"):
                    torch.cuda.set_device(device_id)
                elif device_id == "mps":
                    # MPSè®¾å¤‡è®¾ç½®
                    pass
                
                print(f"âœ… æˆåŠŸè®¾ç½®è®¾å¤‡: {device_id}")
                return True
            except Exception as e:
                print(f"âŒ è®¾ç½®è®¾å¤‡å¤±è´¥: {e}")
                if auto_fallback:
                    fallback_device = validation_result.get("fallback_device") or self.get_fallback_device(device_id)
                    print(f"ğŸ”„ å°è¯•é™çº§åˆ°è®¾å¤‡: {fallback_device}")
                    return self.set_device(fallback_device, auto_fallback=False)
                return False
        else:
            # è®¾å¤‡ä¸å¯ç”¨
            if validation_result["warnings"]:
                for warning in validation_result["warnings"]:
                    print(f"âš ï¸ {warning}")
            
            if auto_fallback and validation_result["fallback_device"]:
                fallback_device = validation_result["fallback_device"]
                print(f"ğŸ”„ è‡ªåŠ¨é™çº§åˆ°è®¾å¤‡: {fallback_device}")
                return self.set_device(fallback_device, auto_fallback=False)
            
            return False
    
    def get_best_available_device(self) -> str:
        """è·å–å½“å‰æœ€ä½³å¯ç”¨è®¾å¤‡"""
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
        priority_devices = []
        
        # æ·»åŠ CUDAè®¾å¤‡
        for device in self.available_devices:
            if device["type"] == "cuda":
                priority_devices.append(device["id"])
        
        # æ·»åŠ MPSè®¾å¤‡
        for device in self.available_devices:
            if device["type"] == "mps":
                priority_devices.append(device["id"])
        
        # æ·»åŠ CPUè®¾å¤‡
        priority_devices.append("cpu")
        
        # é€ä¸ªéªŒè¯è®¾å¤‡å¯ç”¨æ€§
        for device_id in priority_devices:
            validation_result = self.validate_device_availability(device_id)
            if validation_result["available"]:
                return device_id
        
        # å¦‚æœæ‰€æœ‰è®¾å¤‡éƒ½ä¸å¯ç”¨ï¼Œè¿”å›CPUä½œä¸ºæœ€åçš„é€‰æ‹©
        return "cpu"
    
    def get_optimal_batch_size(self, device_id: str, image_size: int = 640) -> int:
        """æ ¹æ®è®¾å¤‡æ¨èæ‰¹æ¬¡å¤§å°"""
        device_info = self.get_device_info(device_id)
        if not device_info:
            return 16
        
        if device_info["type"] == "cpu":
            return 4  # CPUé€šå¸¸ä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡
        elif device_info["type"] == "cuda":
            # æ ¹æ®GPUå†…å­˜æ¨èæ‰¹æ¬¡å¤§å°
            try:
                memory_gb = device_info["memory"] // (1024**3)
                if memory_gb >= 24:
                    return 32
                elif memory_gb >= 12:
                    return 16
                elif memory_gb >= 8:
                    return 8
                else:
                    return 4
            except:
                return 8
        elif device_info["type"] == "mps":
            return 8  # MPSè®¾å¤‡çš„æ¨èæ‰¹æ¬¡
        
        return 16
    
    def validate_device_availability(self, device_id: str) -> Dict:
        """éªŒè¯è®¾å¤‡æ˜¯å¦çœŸå®å¯ç”¨ï¼ˆè¿è¡Œæ—¶æ£€æŸ¥ï¼‰"""
        result = {
            "available": False,
            "device_id": device_id,
            "warnings": [],
            "fallback_device": None
        }
        
        device_info = self.get_device_info(device_id)
        if not device_info:
            result["warnings"].append(f"è®¾å¤‡ {device_id} ä¸å­˜åœ¨")
            result["fallback_device"] = self.get_fallback_device(device_id)
            return result
        
        try:
            # å°è¯•åˆ›å»ºæµ‹è¯•å¼ é‡æ¥éªŒè¯è®¾å¤‡çœŸå®å¯ç”¨æ€§
            if device_id == "cpu":
                test_tensor = torch.randn(2, 2)
                result["available"] = True
                
            elif device_id.startswith("cuda"):
                if torch.cuda.is_available():
                    test_tensor = torch.randn(2, 2).to(device_id)
                    result["available"] = True
                else:
                    result["warnings"].append("CUDAè¿è¡Œæ—¶ä¸å¯ç”¨")
                    result["fallback_device"] = self.get_fallback_device(device_id)
                    
            elif device_id == "mps":
                if torch.backends.mps.is_available():
                    # å°è¯•åˆ›å»ºMPSå¼ é‡
                    test_tensor = torch.randn(2, 2).to("mps")
                    result["available"] = True
                else:
                    result["warnings"].append("MPSè¿è¡Œæ—¶ä¸å¯ç”¨")
                    result["fallback_device"] = self.get_fallback_device(device_id)
            
        except Exception as e:
            result["warnings"].append(f"è®¾å¤‡æµ‹è¯•å¤±è´¥: {str(e)}")
            result["fallback_device"] = self.get_fallback_device(device_id)
        
        return result
    
    def get_fallback_device(self, failed_device_id: str) -> str:
        """è·å–é™çº§è®¾å¤‡"""
        # è®¾å¤‡é™çº§ä¼˜å…ˆçº§ç­–ç•¥
        if failed_device_id.startswith("cuda"):
            # CUDAå¤±è´¥ -> MPS -> CPU
            for device in self.available_devices:
                if device["type"] == "mps" and device["available"]:
                    # éªŒè¯MPSæ˜¯å¦çœŸå®å¯ç”¨
                    mps_check = self.validate_device_availability("mps")
                    if mps_check["available"]:
                        print(f"âš ï¸ CUDAè®¾å¤‡ä¸å¯ç”¨ï¼Œé™çº§åˆ°MPSè®¾å¤‡")
                        return "mps"
            print(f"âš ï¸ CUDAå’ŒMPSè®¾å¤‡éƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°CPU")
            return "cpu"
            
        elif failed_device_id == "mps":
            # MPSå¤±è´¥ -> CUDA -> CPU
            for device in self.available_devices:
                if device["type"] == "cuda" and device["available"]:
                    cuda_check = self.validate_device_availability(device["id"])
                    if cuda_check["available"]:
                        print(f"âš ï¸ MPSè®¾å¤‡ä¸å¯ç”¨ï¼Œé™çº§åˆ°CUDAè®¾å¤‡: {device['id']}")
                        return device["id"]
            print(f"âš ï¸ MPSå’ŒCUDAè®¾å¤‡éƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°CPU")
            return "cpu"
        
        # å…¶ä»–æƒ…å†µæˆ–CPUå¤±è´¥ï¼Œè¿”å›CPUï¼ˆCPUåº”è¯¥æ€»æ˜¯å¯ç”¨çš„ï¼‰
        return "cpu"
    
    def validate_device_compatibility(self, device_id: str) -> Dict:
        """éªŒè¯è®¾å¤‡å…¼å®¹æ€§"""
        result = {
            "compatible": False,
            "warnings": [],
            "recommendations": []
        }
        
        device_info = self.get_device_info(device_id)
        if not device_info:
            result["warnings"].append("è®¾å¤‡ä¸å­˜åœ¨")
            return result
        
        if device_info["type"] == "cuda":
            # æ£€æŸ¥CUDAç‰ˆæœ¬å…¼å®¹æ€§
            try:
                cuda_version = torch.version.cuda
                if cuda_version:
                    result["compatible"] = True
                    result["recommendations"].append(f"CUDAç‰ˆæœ¬: {cuda_version}")
                else:
                    result["warnings"].append("CUDAä¸å¯ç”¨")
            except:
                result["warnings"].append("æ— æ³•æ£€æµ‹CUDAç‰ˆæœ¬")
        
        elif device_info["type"] == "mps":
            # æ£€æŸ¥MPSå…¼å®¹æ€§
            try:
                if torch.backends.mps.is_available():
                    result["compatible"] = True
                    result["recommendations"].append("MPSåŠ é€Ÿå¯ç”¨")
                    
                    # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒMPS
                    pytorch_version = torch.__version__
                    result["recommendations"].append(f"PyTorchç‰ˆæœ¬: {pytorch_version}")
                    
                    # æ£€æŸ¥macOSç‰ˆæœ¬ï¼ˆMPSéœ€è¦macOS 12.3+ï¼‰
                    try:
                        import platform
                        macos_version = platform.mac_ver()[0]
                        if macos_version:
                            result["recommendations"].append(f"macOSç‰ˆæœ¬: {macos_version}")
                    except:
                        pass
                else:
                    result["warnings"].append("MPSä¸å¯ç”¨")
            except:
                result["warnings"].append("æ— æ³•æ£€æµ‹MPSæ”¯æŒ")
        
        else:  # CPU
            result["compatible"] = True
            result["recommendations"].append("CPUè®­ç»ƒç¨³å®šä½†é€Ÿåº¦è¾ƒæ…¢")
        
        return result


# å…¨å±€è®¾å¤‡ç®¡ç†å™¨å®ä¾‹
device_manager = DeviceManager()


def get_device_choices_for_gradio():
    """ä¸ºGradioè·å–è®¾å¤‡é€‰æ‹©"""
    choices = device_manager.get_device_choices()
    descriptions = device_manager.get_device_descriptions()
    
    # åˆ›å»ºé€‰æ‹©é¡¹ï¼Œæ ¼å¼ä¸º "device_id - description"
    formatted_choices = []
    for choice, desc in zip(choices, descriptions):
        formatted_choices.append(f"{choice} - {desc}")
    
    return formatted_choices


def parse_device_choice(choice_str: str) -> str:
    """è§£æGradioè®¾å¤‡é€‰æ‹©"""
    if " - " in choice_str:
        return choice_str.split(" - ")[0]
    return choice_str


def get_recommended_settings(device_id: str, image_size: int = 640) -> Dict:
    """è·å–è®¾å¤‡æ¨èè®¾ç½®"""
    return {
        "batch_size": device_manager.get_optimal_batch_size(device_id, image_size),
        "workers": 4 if device_id == "cpu" else 8,
        "mixed_precision": device_id.startswith("cuda"),
        "pin_memory": device_id != "cpu"
    }
