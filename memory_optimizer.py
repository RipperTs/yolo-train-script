"""
å†…å­˜ä¼˜åŒ–æ¨¡å—
è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜å ç”¨æŒç»­å¢å¤§çš„é—®é¢˜
"""

import gc
import torch
import psutil
import threading
import time
from pathlib import Path
from typing import Dict, Any, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self, enable_monitoring=True, cleanup_interval=10):
        """
        åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–å™¨
        
        Args:
            enable_monitoring: æ˜¯å¦å¯ç”¨å†…å­˜ç›‘æ§
            cleanup_interval: æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
        """
        self.enable_monitoring = enable_monitoring
        self.cleanup_interval = cleanup_interval
        self.monitoring_thread = None
        self.is_monitoring = False
        self.memory_history = []
        self.max_history_size = 100
        
    def start_monitoring(self):
        """å¼€å§‹å†…å­˜ç›‘æ§"""
        if self.enable_monitoring and not self.is_monitoring:
            self.is_monitoring = True
            self.monitoring_thread = threading.Thread(target=self._monitor_memory)
            self.monitoring_thread.daemon = True
            self.monitoring_thread.start()
            logger.info("ğŸ” å†…å­˜ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢å†…å­˜ç›‘æ§"""
        self.is_monitoring = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        logger.info("â¹ï¸ å†…å­˜ç›‘æ§å·²åœæ­¢")
    
    def _monitor_memory(self):
        """å†…å­˜ç›‘æ§çº¿ç¨‹"""
        while self.is_monitoring:
            try:
                memory_info = self.get_memory_info()
                self.memory_history.append(memory_info)
                
                # é™åˆ¶å†å²è®°å½•å¤§å°
                if len(self.memory_history) > self.max_history_size:
                    self.memory_history.pop(0)
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
                if memory_info['ram_percent'] > 85:
                    logger.warning(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_info['ram_percent']:.1f}%")
                    self.force_cleanup()
                
                # æ£€æŸ¥GPUå†…å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if memory_info.get('gpu_memory_percent', 0) > 90:
                    logger.warning(f"âš ï¸ GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_info['gpu_memory_percent']:.1f}%")
                    self.cleanup_gpu_memory()
                
                time.sleep(self.cleanup_interval)
                
            except Exception as e:
                logger.error(f"å†…å­˜ç›‘æ§å‡ºé”™: {e}")
                time.sleep(self.cleanup_interval)
    
    def get_memory_info(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä¿¡æ¯"""
        # ç³»ç»Ÿå†…å­˜
        memory = psutil.virtual_memory()
        info = {
            'ram_used_gb': memory.used / (1024**3),
            'ram_total_gb': memory.total / (1024**3),
            'ram_percent': memory.percent,
            'timestamp': time.time()
        }
        
        # GPUå†…å­˜ï¼ˆCUDAï¼‰
        if torch.cuda.is_available():
            try:
                gpu_memory = torch.cuda.memory_stats()
                allocated = gpu_memory.get('allocated_bytes.all.current', 0)
                reserved = gpu_memory.get('reserved_bytes.all.current', 0)
                total = torch.cuda.get_device_properties(0).total_memory
                
                info.update({
                    'gpu_allocated_gb': allocated / (1024**3),
                    'gpu_reserved_gb': reserved / (1024**3),
                    'gpu_total_gb': total / (1024**3),
                    'gpu_memory_percent': (reserved / total) * 100 if total > 0 else 0
                })
            except Exception as e:
                logger.debug(f"è·å–GPUå†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        
        # MPSå†…å­˜ï¼ˆApple Siliconï¼‰
        if torch.backends.mps.is_available():
            try:
                # MPSå†…å­˜ä¿¡æ¯è¾ƒéš¾è·å–ï¼Œä½¿ç”¨ç³»ç»Ÿå†…å­˜ä½œä¸ºè¿‘ä¼¼
                info['mps_available'] = True
            except Exception as e:
                logger.debug(f"è·å–MPSå†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        
        return info
    
    def cleanup_python_memory(self):
        """æ¸…ç†Pythonå†…å­˜"""
        try:
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            collected = gc.collect()
            logger.debug(f"ğŸ§¹ Pythonåƒåœ¾å›æ”¶: æ¸…ç†äº† {collected} ä¸ªå¯¹è±¡")
            
            # æ¸…ç†æœªå¼•ç”¨çš„å¾ªç¯
            gc.collect()
            
        except Exception as e:
            logger.error(f"Pythonå†…å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def cleanup_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        try:
            if torch.cuda.is_available():
                # æ¸…ç©ºCUDAç¼“å­˜
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.debug("ğŸ§¹ CUDAå†…å­˜ç¼“å­˜å·²æ¸…ç†")
            
            if torch.backends.mps.is_available():
                # MPSå†…å­˜æ¸…ç†
                torch.mps.empty_cache()
                logger.debug("ğŸ§¹ MPSå†…å­˜ç¼“å­˜å·²æ¸…ç†")
                
        except Exception as e:
            logger.error(f"GPUå†…å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def force_cleanup(self):
        """å¼ºåˆ¶å†…å­˜æ¸…ç†"""
        logger.info("ğŸ§¹ æ‰§è¡Œå¼ºåˆ¶å†…å­˜æ¸…ç†...")
        
        # æ¸…ç†Pythonå†…å­˜
        self.cleanup_python_memory()
        
        # æ¸…ç†GPUå†…å­˜
        self.cleanup_gpu_memory()
        
        # å†æ¬¡åƒåœ¾å›æ”¶
        gc.collect()
        
        logger.info("âœ… å¼ºåˆ¶å†…å­˜æ¸…ç†å®Œæˆ")
    
    def optimize_training_config(self, device: str, current_config: Dict) -> Dict:
        """æ ¹æ®å†…å­˜æƒ…å†µä¼˜åŒ–è®­ç»ƒé…ç½®"""
        memory_info = self.get_memory_info()
        optimized_config = current_config.copy()
        
        # æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´æ‰¹æ¬¡å¤§å°
        if memory_info['ram_percent'] > 80:
            # å†…å­˜ç´§å¼ ï¼Œå‡å°æ‰¹æ¬¡å¤§å°
            current_batch = optimized_config.get('batch_size', 16)
            new_batch = max(1, current_batch // 2)
            optimized_config['batch_size'] = new_batch
            logger.warning(f"âš ï¸ å†…å­˜ç´§å¼ ï¼Œæ‰¹æ¬¡å¤§å°ä» {current_batch} è°ƒæ•´ä¸º {new_batch}")
        
        # æ ¹æ®è®¾å¤‡ç±»å‹ä¼˜åŒ–
        if device == "mps":
            # MPSè®¾å¤‡ä¼˜åŒ–
            optimized_config.update({
                'workers': min(4, optimized_config.get('workers', 8)),  # é™åˆ¶workeræ•°é‡
                'pin_memory': False,  # MPSä¸éœ€è¦pin_memory
                'persistent_workers': True,  # ä½¿ç”¨æŒä¹…åŒ–worker
                'cache': False,  # ç¦ç”¨æ•°æ®é›†ç¼“å­˜ä»¥èŠ‚çœå†…å­˜
            })
        elif device.startswith("cuda"):
            # CUDAè®¾å¤‡ä¼˜åŒ–
            gpu_memory_percent = memory_info.get('gpu_memory_percent', 0)
            if gpu_memory_percent > 80:
                # GPUå†…å­˜ç´§å¼ 
                current_batch = optimized_config.get('batch_size', 16)
                new_batch = max(1, current_batch // 2)
                optimized_config['batch_size'] = new_batch
                optimized_config['cache'] = False  # ç¦ç”¨ç¼“å­˜
                logger.warning(f"âš ï¸ GPUå†…å­˜ç´§å¼ ï¼Œæ‰¹æ¬¡å¤§å°è°ƒæ•´ä¸º {new_batch}")
        else:  # CPU
            # CPUè®¾å¤‡ä¼˜åŒ–
            optimized_config.update({
                'workers': min(2, optimized_config.get('workers', 8)),  # CPUé™åˆ¶workeræ•°é‡
                'cache': False,  # CPUå†…å­˜æœ‰é™ï¼Œç¦ç”¨ç¼“å­˜
                'pin_memory': False,  # CPUä¸éœ€è¦pin_memory
            })
        
        return optimized_config
    
    def get_memory_report(self) -> str:
        """ç”Ÿæˆå†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
        if not self.memory_history:
            return "ğŸ“Š æš‚æ— å†…å­˜ç›‘æ§æ•°æ®"
        
        current = self.memory_history[-1]
        report = [
            "ğŸ“Š å†…å­˜ä½¿ç”¨æŠ¥å‘Š",
            "=" * 40,
            f"ğŸ–¥ï¸  ç³»ç»Ÿå†…å­˜: {current['ram_used_gb']:.1f}GB / {current['ram_total_gb']:.1f}GB ({current['ram_percent']:.1f}%)"
        ]
        
        if 'gpu_allocated_gb' in current:
            report.append(f"ğŸ® GPUå†…å­˜: {current['gpu_allocated_gb']:.1f}GB / {current['gpu_total_gb']:.1f}GB ({current['gpu_memory_percent']:.1f}%)")
        
        if 'mps_available' in current:
            report.append("ğŸ MPSè®¾å¤‡: å¯ç”¨")
        
        # å†…å­˜è¶‹åŠ¿åˆ†æ
        if len(self.memory_history) >= 2:
            prev = self.memory_history[-2]
            ram_trend = current['ram_percent'] - prev['ram_percent']
            trend_symbol = "ğŸ“ˆ" if ram_trend > 0 else "ğŸ“‰" if ram_trend < 0 else "â¡ï¸"
            report.append(f"{trend_symbol} å†…å­˜è¶‹åŠ¿: {ram_trend:+.1f}%")
        
        return "\n".join(report)


class TrainingMemoryManager:
    """è®­ç»ƒå†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, optimizer: MemoryOptimizer):
        self.optimizer = optimizer
        self.epoch_cleanup_interval = 5  # æ¯5ä¸ªepochæ¸…ç†ä¸€æ¬¡
        self.last_cleanup_epoch = 0
    
    def pre_training_setup(self, config: Dict) -> Dict:
        """è®­ç»ƒå‰è®¾ç½®"""
        logger.info("ğŸš€ è®­ç»ƒå‰å†…å­˜ä¼˜åŒ–è®¾ç½®...")
        
        # å¯åŠ¨å†…å­˜ç›‘æ§
        self.optimizer.start_monitoring()
        
        # åˆå§‹å†…å­˜æ¸…ç†
        self.optimizer.force_cleanup()
        
        # ä¼˜åŒ–é…ç½®
        device = config.get('device', 'cpu')
        optimized_config = self.optimizer.optimize_training_config(device, config)
        
        logger.info("âœ… è®­ç»ƒå‰å†…å­˜ä¼˜åŒ–å®Œæˆ")
        return optimized_config
    
    def post_training_cleanup(self):
        """è®­ç»ƒåæ¸…ç†"""
        logger.info("ğŸ§¹ è®­ç»ƒåå†…å­˜æ¸…ç†...")
        
        # åœæ­¢ç›‘æ§
        self.optimizer.stop_monitoring()
        
        # å¼ºåˆ¶æ¸…ç†
        self.optimizer.force_cleanup()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.optimizer.get_memory_report()
        logger.info(f"\n{report}")
        
        logger.info("âœ… è®­ç»ƒåå†…å­˜æ¸…ç†å®Œæˆ")
    
    def epoch_cleanup(self, current_epoch: int):
        """æ¯ä¸ªepochçš„å†…å­˜æ¸…ç†"""
        if current_epoch - self.last_cleanup_epoch >= self.epoch_cleanup_interval:
            logger.debug(f"ğŸ§¹ Epoch {current_epoch} å†…å­˜æ¸…ç†...")
            self.optimizer.cleanup_python_memory()
            self.optimizer.cleanup_gpu_memory()
            self.last_cleanup_epoch = current_epoch


# å…¨å±€å†…å­˜ä¼˜åŒ–å™¨å®ä¾‹
memory_optimizer = MemoryOptimizer()
training_memory_manager = TrainingMemoryManager(memory_optimizer)


def optimize_ultralytics_training_args(train_args: Dict, device: str) -> Dict:
    """ä¼˜åŒ–Ultralyticsè®­ç»ƒå‚æ•°ä»¥å‡å°‘å†…å­˜ä½¿ç”¨"""
    optimized_args = train_args.copy()
    
    # åŸºç¡€å†…å­˜ä¼˜åŒ–å‚æ•°
    memory_optimizations = {
        'cache': False,  # ç¦ç”¨æ•°æ®é›†ç¼“å­˜
        'save_period': max(10, train_args.get('save_period', 10)),  # å‡å°‘ä¿å­˜é¢‘ç‡
        'plots': False,  # ç¦ç”¨è®­ç»ƒå›¾è¡¨ç”Ÿæˆ
        'val': True,  # ä¿æŒéªŒè¯ä½†å¯èƒ½å‡å°‘é¢‘ç‡
    }
    
    # è®¾å¤‡ç‰¹å®šä¼˜åŒ–
    if device == "mps":
        memory_optimizations.update({
            'amp': False,  # MPSå¯èƒ½ä¸å®Œå…¨æ”¯æŒæ··åˆç²¾åº¦
            'workers': min(4, train_args.get('workers', 8)),
        })
    elif device.startswith("cuda"):
        memory_optimizations.update({
            'amp': True,  # CUDAæ”¯æŒæ··åˆç²¾åº¦ï¼Œå¯ä»¥èŠ‚çœå†…å­˜
        })
    else:  # CPU
        memory_optimizations.update({
            'amp': False,  # CPUä¸æ”¯æŒæ··åˆç²¾åº¦
            'workers': min(2, train_args.get('workers', 8)),
        })
    
    optimized_args.update(memory_optimizations)
    return optimized_args


if __name__ == "__main__":
    # æµ‹è¯•å†…å­˜ä¼˜åŒ–å™¨
    optimizer = MemoryOptimizer()
    
    print("ğŸ” å¼€å§‹å†…å­˜ç›‘æ§æµ‹è¯•...")
    optimizer.start_monitoring()
    
    # æ¨¡æ‹Ÿä¸€äº›å†…å­˜ä½¿ç”¨
    time.sleep(5)
    
    # ç”ŸæˆæŠ¥å‘Š
    print(optimizer.get_memory_report())
    
    # åœæ­¢ç›‘æ§
    optimizer.stop_monitoring()
    
    print("âœ… å†…å­˜ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ")
